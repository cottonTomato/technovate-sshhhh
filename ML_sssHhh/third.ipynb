{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.cluster import KMeans\n",
    "from prophet import Prophet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Union, Tuple, Optional\n",
    "import joblib\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesPreprocessor:\n",
    "    \"\"\"Handles time series data preprocessing and feature engineering\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def create_temporal_features(self) -> pd.DataFrame:\n",
    "        \"\"\"Create temporal features from datetime index\"\"\"\n",
    "        df = self.data.copy()\n",
    "        df['hour'] = df.index.hour\n",
    "        df['day_of_week'] = df.index.dayofweek\n",
    "        df['month'] = df.index.month\n",
    "        df['is_weekend'] = df.index.dayofweek.isin([5, 6]).astype(int)\n",
    "        return df\n",
    "        \n",
    "    def create_lag_features(self, columns: List[str], lags: List[int]) -> pd.DataFrame:\n",
    "        \"\"\"Create lagged features for specified columns\"\"\"\n",
    "        df = self.data.copy()\n",
    "        for col in columns:\n",
    "            for lag in lags:\n",
    "                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "        return df\n",
    "        \n",
    "    def create_rolling_features(self, columns: List[str], windows: List[int]) -> pd.DataFrame:\n",
    "        \"\"\"Create rolling statistics features\"\"\"\n",
    "        df = self.data.copy()\n",
    "        for col in columns:\n",
    "            for window in windows:\n",
    "                df[f'{col}_rolling_mean_{window}'] = df[col].rolling(window=window).mean()\n",
    "                df[f'{col}_rolling_std_{window}'] = df[col].rolling(window=window).std()\n",
    "        return df\n",
    "    def preprocess_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess data by creating temporal, lag, and rolling features\"\"\"\n",
    "        df = self.create_temporal_features()\n",
    "        df = self.create_lag_features(columns=['total_power_consumption'], lags=[1, 2, 3])\n",
    "        df = self.create_rolling_features(columns=['total_power_consumption'], windows=[3, 7])\n",
    "        df.dropna(inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsumptionPredictor:\n",
    "    \"\"\"Handles time series prediction using various models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prophet_model = None\n",
    "        self.lstm_model = None\n",
    "        self.rf_model = None\n",
    "        self.catboost_model = None\n",
    "        \n",
    "    def train_prophet(self, data: pd.DataFrame, target_col: str) -> Dict:\n",
    "        \"\"\"Train Facebook Prophet model for time series prediction\"\"\"\n",
    "        try:\n",
    "            df = data.reset_index()\n",
    "            df = df.rename(columns={'datetime': 'ds', target_col: 'y'})\n",
    "            \n",
    "            self.prophet_model = Prophet(\n",
    "                yearly_seasonality=True,\n",
    "                weekly_seasonality=True,\n",
    "                daily_seasonality=True\n",
    "            )\n",
    "            self.prophet_model.fit(df)\n",
    "            \n",
    "            # Generate forecast\n",
    "            future = self.prophet_model.make_future_dataframe(periods=30)\n",
    "            forecast = self.prophet_model.predict(future)\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'forecast': forecast.to_dict('records'),\n",
    "                'components': self.prophet_model.plot_components(forecast).to_dict()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prophet training error: {str(e)}\")\n",
    "            return {'status': 'error', 'message': str(e)}\n",
    "            \n",
    "    def train_lstm(self, data: pd.DataFrame, target_col: str, lookback: int = 24) -> Dict:\n",
    "        \"\"\"Train LSTM model for time series prediction\"\"\"\n",
    "        try:\n",
    "            # Prepare sequences\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_data = scaler.fit_transform(data[[target_col]])\n",
    "            \n",
    "            X, y = [], []\n",
    "            for i in range(lookback, len(scaled_data)):\n",
    "                X.append(scaled_data[i-lookback:i])\n",
    "                y.append(scaled_data[i])\n",
    "            X, y = np.array(X), np.array(y)\n",
    "            \n",
    "            # Split data\n",
    "            train_size = int(len(X) * 0.8)\n",
    "            X_train, X_test = X[:train_size], X[train_size:]\n",
    "            y_train, y_test = y[:train_size], y[train_size:]\n",
    "            \n",
    "            # Build model\n",
    "            self.lstm_model = Sequential([\n",
    "                LSTM(50, activation='relu', input_shape=(lookback, 1), return_sequences=True),\n",
    "                Dropout(0.2),\n",
    "                LSTM(50, activation='relu'),\n",
    "                Dropout(0.2),\n",
    "                Dense(1)\n",
    "            ])\n",
    "            \n",
    "            self.lstm_model.compile(optimizer='adam', loss='mse')\n",
    "            \n",
    "            # Train model\n",
    "            history = self.lstm_model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                validation_split=0.1,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = self.lstm_model.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'mse': mse,\n",
    "                'history': history.history,\n",
    "                'scaler': scaler\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"LSTM training error: {str(e)}\")\n",
    "            return {'status': 'error', 'message': str(e)}\n",
    "    def train_arima(self, data: pd.DataFrame, target_col: str) -> Dict:\n",
    "        \"\"\"Train ARIMA model for time series prediction\"\"\"\n",
    "        try:\n",
    "            from statsmodels.tsa.arima.model import ARIMA\n",
    "            \n",
    "            model = ARIMA(data[target_col], order=(5, 1, 0))\n",
    "            model_fit = model.fit()\n",
    "            \n",
    "            forecast = model_fit.forecast(steps=30)\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'forecast': forecast.tolist()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ARIMA training error: {str(e)}\")\n",
    "            return {'status': 'error', 'message': str(e)}\n",
    "    def train_catboost(self, data: pd.DataFrame, target_col: str) -> Dict:\n",
    "        \"\"\"Train CatBoost model for time series prediction\"\"\"\n",
    "        try:\n",
    "            X = data.drop(columns=[target_col])\n",
    "            y = data[target_col]\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            self.catboost_model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, verbose=0)\n",
    "            self.catboost_model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred = self.catboost_model.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'mse': mse,\n",
    "                'feature_importances': self.catboost_model.get_feature_importance().tolist()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"CatBoost training error: {str(e)}\")\n",
    "            return {'status': 'error', 'message': str(e)}\n",
    "    def cluster_consumption_patterns(self, data: pd.DataFrame, n_clusters: int = 3) -> Dict:\n",
    "        \"\"\"Cluster consumption patterns using KMeans\"\"\"\n",
    "        try:\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            clusters = kmeans.fit_predict(data)\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'clusters': clusters.tolist()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Clustering error: {str(e)}\")\n",
    "            return {'status': 'error', 'message': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector:\n",
    "    \"\"\"Handles anomaly detection in consumption patterns\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "    def detect_anomalies(self, data: pd.DataFrame, contamination: float = 0.1) -> Dict:\n",
    "        \"\"\"Detect anomalies using Isolation Forest\"\"\"\n",
    "        try:\n",
    "            self.model = IsolationForest(\n",
    "                contamination=contamination,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # Fit and predict\n",
    "            predictions = self.model.fit_predict(data)\n",
    "            anomaly_mask = predictions == -1\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'anomalies': {\n",
    "                    'indices': data.index[anomaly_mask].tolist(),\n",
    "                    'values': data[anomaly_mask].to_dict('records')\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Anomaly detection error: {str(e)}\")\n",
    "            return {'status': 'error', 'message': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcoScoreCalculator:\n",
    "    \"\"\"Calculates Eco Score and provides optimization suggestions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "    def calculate_eco_score(self, data: pd.DataFrame, benchmarks: Dict[str, float]) -> Dict:\n",
    "        \"\"\"Calculate Eco Score based on benchmarks\"\"\"\n",
    "        try:\n",
    "            scores = {}\n",
    "            for col in data.columns:\n",
    "                scores[col] = 100 - ((data[col].mean() / benchmarks[col]) * 100)\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'eco_scores': scores\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Eco Score calculation error: {str(e)}\")\n",
    "            return {'status': 'error', 'message': str(e)}\n",
    "    def provide_optimization_suggestions(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Provide optimization suggestions based on usage patterns\"\"\"\n",
    "        try:\n",
    "            suggestions = []\n",
    "            if data['consumption'].mean() > data['consumption'].quantile(0.75):\n",
    "                suggestions.append(\"Consider reducing peak hour usage.\")\n",
    "            if data['consumption'].std() > data['consumption'].mean() * 0.5:\n",
    "                suggestions.append(\"Usage is highly variable, consider more consistent usage patterns.\")\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'suggestions': suggestions\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Optimization suggestions error: {str(e)}\")\n",
    "            return {'status': 'error', 'message': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BehavioralSegmentation:\n",
    "    \"\"\"Segments users based on their consumption habits\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "    def segment_users(self, data: pd.DataFrame, n_segments: int = 3) -> Dict:\n",
    "        \"\"\"Segment users using KMeans clustering\"\"\"\n",
    "        try:\n",
    "            kmeans = KMeans(n_segments, random_state=42)\n",
    "            segments = kmeans.fit_predict(data)\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'segments': segments.tolist()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Segmentation error: {str(e)}\")\n",
    "            return {'status': 'error', 'message': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commenting out the Flask part<br>\n",
    "from flask import Flask, request, jsonify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@app.route('/predict', methods=['POST'])<br>\n",
    "def predict():<br>\n",
    "    data = request.json<br>\n",
    "    df = pd.DataFrame(data['data'])<br>\n",
    "    target_col = data['target_col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     predictor = ConsumptionPredictor()\n",
    "#     result = predictor.train_prophet(df, target_col)\n",
    "#     return jsonify(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@app.route('/anomaly', methods=['POST'])<br>\n",
    "def anomaly():<br>\n",
    "    data = request.json<br>\n",
    "    df = pd.DataFrame(data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     detector = AnomalyDetector()\n",
    "#     result = detector.detect_anomalies(df)\n",
    "#     return jsonify(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@app.route('/eco_score', methods=['POST'])<br>\n",
    "def eco_score():<br>\n",
    "    data = request.json<br>\n",
    "    df = pd.DataFrame(data['data'])<br>\n",
    "    benchmarks = data['benchmarks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     calculator = EcoScoreCalculator()\n",
    "#     result = calculator.calculate_eco_score(df, benchmarks)\n",
    "#     return jsonify(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@app.route('/optimize', methods=['POST'])<br>\n",
    "def optimize():<br>\n",
    "    data = request.json<br>\n",
    "    df = pd.DataFrame(data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     calculator = EcoScoreCalculator()\n",
    "#     result = calculator.provide_optimization_suggestions(df)\n",
    "#     return jsonify(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@app.route('/segment', methods=['POST'])<br>\n",
    "def segment():<br>\n",
    "    data = request.json<br>\n",
    "    df = pd.DataFrame(data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     segmenter = BehavioralSegmentation()\n",
    "#     result = segmenter.segment_users(df)\n",
    "#     return jsonify(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@app.route('/predict_catboost', methods=['POST'])<br>\n",
    "def predict_catboost():<br>\n",
    "    data = request.json<br>\n",
    "    df = pd.DataFrame(data['data'])<br>\n",
    "    target_col = data['target_col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     predictor = ConsumptionPredictor()\n",
    "#     result = predictor.train_catboost(df, target_col)\n",
    "#     return jsonify(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@app.route('/individual_dash', methods=['POST'])<br>\n",
    "def individual_dash():<br>\n",
    "    data = request.json<br>\n",
    "    df = pd.DataFrame(data['data'])<br>\n",
    "    target_col = data['target_col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     predictor = ConsumptionPredictor()\n",
    "#     result = predictor.train_prophet(df, target_col)\n",
    "#     return jsonify(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@app.route('/family_dash', methods=['POST'])<br>\n",
    "def family_dash():<br>\n",
    "    data = request.json<br>\n",
    "    df = pd.DataFrame(data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     segmenter = BehavioralSegmentation()\n",
    "#     result = segmenter.segment_users(df)\n",
    "#     return jsonify(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@app.route('/appliance_dash', methods=['POST'])<br>\n",
    "def appliance_dash():<br>\n",
    "    data = request.json<br>\n",
    "    df = pd.DataFrame(data['data'])<br>\n",
    "    target_col = data['target_col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     predictor = ConsumptionPredictor()\n",
    "#     result = predictor.train_lstm(df, target_col)\n",
    "#     return jsonify(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':<br>\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis and Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamps_to_strings(data: Union[Dict, List]) -> Union[Dict, List]:\n",
    "    \"\"\"Convert all Timestamp objects in a nested dictionary or list to strings.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, pd.Timestamp):\n",
    "                data[key] = value.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            elif isinstance(value, (dict, list)):\n",
    "                data[key] = convert_timestamps_to_strings(value)\n",
    "    elif isinstance(data, list):\n",
    "        for i in range(len(data)):\n",
    "            if isinstance(data[i], pd.Timestamp):\n",
    "                data[i] = data[i].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            elif isinstance(data[i], (dict, list)):\n",
    "                data[i] = convert_timestamps_to_strings(data[i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_individuals(data: pd.DataFrame, target_col: str):\n",
    "    \"\"\"Analyze and generate graphs for each individual\"\"\"\n",
    "    individuals = data['individual_id'].unique()\n",
    "    results = []\n",
    "    for individual in individuals:\n",
    "        individual_data = data[data['individual_id'] == individual]\n",
    "        \n",
    "        # Generate consumption graph\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(individual_data.index, individual_data[target_col], label=f'{individual} Consumption')\n",
    "        plt.title(f'{individual} Consumption Over Time')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Consumption')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'OUTPUT_2/{individual}_consumption.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Predict consumption using Prophet\n",
    "        predictor = ConsumptionPredictor()\n",
    "        prediction_result = predictor.train_prophet(individual_data, target_col)\n",
    "        \n",
    "        # Detect anomalies\n",
    "        detector = AnomalyDetector()\n",
    "        anomaly_result = detector.detect_anomalies(individual_data[[target_col]])\n",
    "        \n",
    "        results.append({\n",
    "            'individual': individual,\n",
    "            'prediction': prediction_result,\n",
    "            'anomalies': anomaly_result\n",
    "        })\n",
    "    \n",
    "    # Convert Timestamps to strings for JSON serialization\n",
    "    results = convert_timestamps_to_strings(results)\n",
    "    \n",
    "    with open('OUTPUT_2/individual_analysis.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_family(data: pd.DataFrame, target_col: str):\n",
    "    \"\"\"Analyze and generate graphs for the entire family\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for individual in data['individual_id'].unique():\n",
    "        individual_data = data[data['individual_id'] == individual]\n",
    "        plt.plot(individual_data.index, individual_data[target_col], label=f'{individual} Consumption')\n",
    "    plt.title('Family Consumption Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Consumption')\n",
    "    plt.legend()\n",
    "    plt.savefig('OUTPUT_2/family_consumption.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Predict consumption using Prophet\n",
    "    predictor = ConsumptionPredictor()\n",
    "    prediction_result = predictor.train_prophet(data, target_col)\n",
    "    \n",
    "    # Detect anomalies\n",
    "    detector = AnomalyDetector()\n",
    "    anomaly_result = detector.detect_anomalies(data[[target_col]])\n",
    "    \n",
    "    results = {\n",
    "        'prediction': prediction_result,\n",
    "        'anomalies': anomaly_result\n",
    "    }\n",
    "    \n",
    "    # Convert Timestamps to strings for JSON serialization\n",
    "    results = convert_timestamps_to_strings(results)\n",
    "    \n",
    "    with open('OUTPUT_2/family_analysis.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_appliances(data: pd.DataFrame, appliances: List[str]):\n",
    "    \"\"\"Analyze and generate graphs for top appliances\"\"\"\n",
    "    results = []\n",
    "    for appliance in appliances:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(data.index, data[appliance], label=f'{appliance} Usage')\n",
    "        plt.title(f'{appliance} Usage Over Time')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Usage')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'OUTPUT_2/{appliance}_usage.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Predict usage using Prophet\n",
    "        predictor = ConsumptionPredictor()\n",
    "        prediction_result = predictor.train_prophet(data, appliance)\n",
    "        \n",
    "        # Detect anomalies\n",
    "        detector = AnomalyDetector()\n",
    "        anomaly_result = detector.detect_anomalies(data[[appliance]])\n",
    "        \n",
    "        results.append({\n",
    "            'appliance': appliance,\n",
    "            'prediction': prediction_result,\n",
    "            'anomalies': anomaly_result\n",
    "        })\n",
    "    \n",
    "    # Convert Timestamps to strings for JSON serialization\n",
    "    results = convert_timestamps_to_strings(results)\n",
    "    \n",
    "    with open('OUTPUT_2/appliance_analysis.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load your data here\n",
    "    data = pd.read_csv(r'output/indian_family_lifestyle_data.csv', index_col='datetime', parse_dates=True)\n",
    "    \n",
    "    # Preprocess data\n",
    "    preprocessor = TimeSeriesPreprocessor(data)\n",
    "    processed_data = preprocessor.preprocess_data()\n",
    "    \n",
    "    # Analyze individuals\n",
    "    analyze_individuals(processed_data, target_col='total_power_consumption')\n",
    "    \n",
    "    # Analyze family\n",
    "    analyze_family(processed_data, target_col='total_power_consumption')\n",
    "    \n",
    "    # Analyze top appliances\n",
    "    top_appliances = ['mixer_grinder_usage', 'pressure_cooker_usage', 'microwave_usage']\n",
    "    analyze_appliances(processed_data, top_appliances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_chart(data: pd.Series, title: str, filename: str):\n",
    "    \"\"\"Plot a pie chart and save it to a file.\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    data.value_counts().plot.pie(autopct='%1.1f%%')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_chart(data: pd.Series, title: str, xlabel: str, ylabel: str, filename: str):\n",
    "    \"\"\"Plot a bar chart and save it to a file.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    data.value_counts().plot.bar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_chart(data: pd.DataFrame, title: str, xlabel: str, ylabel: str, filename: str):\n",
    "    \"\"\"Plot a line chart and save it to a file.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for col in data.columns:\n",
    "        plt.plot(data.index, data[col], label=col)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_chart(data: pd.DataFrame, x_col: str, y_col: str, z_col: str, title: str, filename: str):\n",
    "    \"\"\"Plot a 3D chart and save it to a file.\"\"\"\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(data[x_col], data[y_col], data[z_col])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_col)\n",
    "    ax.set_ylabel(y_col)\n",
    "    ax.set_zlabel(z_col)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_individuals(data: pd.DataFrame, target_col: str):\n",
    "    \"\"\"Analyze and generate graphs for each individual\"\"\"\n",
    "    individuals = data['individual_id'].unique()\n",
    "    results = []\n",
    "    for individual in individuals:\n",
    "        individual_data = data[data['individual_id'] == individual]\n",
    "        \n",
    "        # Generate consumption graph\n",
    "        plot_line_chart(individual_data[[target_col]], f'{individual} Consumption Over Time', 'Time', 'Consumption', f'OUTPUT_2/{individual}_consumption.png')\n",
    "        \n",
    "        # Predict consumption using Prophet\n",
    "        predictor = ConsumptionPredictor()\n",
    "        prediction_result = predictor.train_prophet(individual_data, target_col)\n",
    "        \n",
    "        # Detect anomalies\n",
    "        detector = AnomalyDetector()\n",
    "        anomaly_result = detector.detect_anomalies(individual_data[[target_col]])\n",
    "        \n",
    "        results.append({\n",
    "            'individual': individual,\n",
    "            'prediction': prediction_result,\n",
    "            'anomalies': anomaly_result\n",
    "        })\n",
    "    \n",
    "    # Convert Timestamps to strings for JSON serialization\n",
    "    results = convert_timestamps_to_strings(results)\n",
    "    \n",
    "    with open('OUTPUT_2/individual_analysis.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_family(data: pd.DataFrame, target_col: str):\n",
    "    \"\"\"Analyze and generate graphs for the entire family\"\"\"\n",
    "    plot_line_chart(data.pivot(columns='individual_id', values=target_col), 'Family Consumption Over Time', 'Time', 'Consumption', 'OUTPUT_2/family_consumption.png')\n",
    "    \n",
    "    # Predict consumption using Prophet\n",
    "    predictor = ConsumptionPredictor()\n",
    "    prediction_result = predictor.train_prophet(data, target_col)\n",
    "    \n",
    "    # Detect anomalies\n",
    "    detector = AnomalyDetector()\n",
    "    anomaly_result = detector.detect_anomalies(data[[target_col]])\n",
    "    \n",
    "    results = {\n",
    "        'prediction': prediction_result,\n",
    "        'anomalies': anomaly_result\n",
    "    }\n",
    "    \n",
    "    # Convert Timestamps to strings for JSON serialization\n",
    "    results = convert_timestamps_to_strings(results)\n",
    "    \n",
    "    with open('OUTPUT_2/family_analysis.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_appliances(data: pd.DataFrame, appliances: List[str]):\n",
    "    \"\"\"Analyze and generate graphs for top appliances\"\"\"\n",
    "    results = []\n",
    "    for appliance in appliances:\n",
    "        plot_line_chart(data[[appliance]], f'{appliance} Usage Over Time', 'Time', 'Usage', f'OUTPUT_2/{appliance}_usage.png')\n",
    "        \n",
    "        # Predict usage using Prophet\n",
    "        predictor = ConsumptionPredictor()\n",
    "        prediction_result = predictor.train_prophet(data, appliance)\n",
    "        \n",
    "        # Detect anomalies\n",
    "        detector = AnomalyDetector()\n",
    "        anomaly_result = detector.detect_anomalies(data[[appliance]])\n",
    "        \n",
    "        results.append({\n",
    "            'appliance': appliance,\n",
    "            'prediction': prediction_result,\n",
    "            'anomalies': anomaly_result\n",
    "        })\n",
    "    \n",
    "    # Convert Timestamps to strings for JSON serialization\n",
    "    results = convert_timestamps_to_strings(results)\n",
    "    \n",
    "    with open('OUTPUT_2/appliance_analysis.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_behavioral_segmentation(data: pd.DataFrame):\n",
    "    \"\"\"Analyze and generate graphs for behavioral segmentation\"\"\"\n",
    "    segmenter = BehavioralSegmentation()\n",
    "    numeric_data = data.select_dtypes(include=[np.number])  # Ensure data is numeric\n",
    "    segmentation_result = segmenter.segment_users(numeric_data)\n",
    "    \n",
    "    # Plot segmentation results\n",
    "    plot_pie_chart(pd.Series(segmentation_result['segments']), 'Behavioral Segmentation', 'OUTPUT_2/behavioral_segmentation.png')\n",
    "    \n",
    "    return segmentation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eco_score(data: pd.DataFrame, benchmarks: Dict[str, float]):\n",
    "    \"\"\"Calculate and generate graphs for Eco Score\"\"\"\n",
    "    calculator = EcoScoreCalculator()\n",
    "    eco_score_result = calculator.calculate_eco_score(data, benchmarks)\n",
    "    \n",
    "    # Plot Eco Score results\n",
    "    plot_bar_chart(pd.Series(eco_score_result['eco_scores']), 'Eco Scores', 'Category', 'Score', 'OUTPUT_2/eco_scores.png')\n",
    "    \n",
    "    return eco_score_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_optimization_suggestions(data: pd.DataFrame):\n",
    "    \"\"\"Provide optimization suggestions based on usage patterns\"\"\"\n",
    "    calculator = EcoScoreCalculator()\n",
    "    suggestions_result = calculator.provide_optimization_suggestions(data)\n",
    "    \n",
    "    return suggestions_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load your data here\n",
    "    data = pd.read_csv(r'output/indian_family_lifestyle_data.csv', index_col='datetime', parse_dates=True)\n",
    "    \n",
    "    # Preprocess data\n",
    "    preprocessor = TimeSeriesPreprocessor(data)\n",
    "    processed_data = preprocessor.preprocess_data()\n",
    "    \n",
    "    # Analyze individuals\n",
    "    analyze_individuals(processed_data, target_col='total_power_consumption')\n",
    "    \n",
    "    # Analyze family\n",
    "    analyze_family(processed_data, target_col='total_power_consumption')\n",
    "    \n",
    "    # Analyze top appliances\n",
    "    top_appliances = ['mixer_grinder_usage', 'pressure_cooker_usage', 'microwave_usage']\n",
    "    analyze_appliances(processed_data, top_appliances)\n",
    "    \n",
    "    # Behavioral segmentation\n",
    "    segmentation_result = analyze_behavioral_segmentation(processed_data)\n",
    "    \n",
    "    # Calculate Eco Score\n",
    "    benchmarks = {'total_power_consumption': 1000, 'mixer_grinder_usage': 50, 'pressure_cooker_usage': 30, 'microwave_usage': 20}\n",
    "    eco_score_result = calculate_eco_score(processed_data, benchmarks)\n",
    "    \n",
    "    # Provide optimization suggestions\n",
    "    suggestions_result = provide_optimization_suggestions(processed_data)\n",
    "    \n",
    "    # Store results in JSON\n",
    "    results = {\n",
    "        'behavioral_segmentation': segmentation_result,\n",
    "        'eco_score': eco_score_result,\n",
    "        'optimization_suggestions': suggestions_result\n",
    "    }\n",
    "    \n",
    "    with open('OUTPUT_2/analysis_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Behavioral Segmentation Result:\", segmentation_result)\n",
    "    print(\"Eco Score Result:\", eco_score_result)\n",
    "    print(\"Optimization Suggestions:\", suggestions_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
